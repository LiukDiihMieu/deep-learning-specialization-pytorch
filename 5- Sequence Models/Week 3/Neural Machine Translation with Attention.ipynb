{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29b1201f-f69d-42c7-a9ca-de274a30746a",
   "metadata": {
    "id": "adTDe2CTh3MU"
   },
   "source": [
    "# Neural Machine Translation\n",
    "\n",
    "Welcome to your first programming assignment for this week! \n",
    "\n",
    "* You will build a Neural Machine Translation (NMT) model to translate human-readable dates (\"25th of June, 2009\") into machine-readable dates (\"2009-06-25\"). \n",
    "* You will do this using an attention model, one of the most sophisticated sequence-to-sequence models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8c1de49-de3f-4d6b-8c1b-ec303633ed78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nmt_utils import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5c241f-a2fd-42e7-8d76-7c66cc2871c7",
   "metadata": {
    "id": "J0pkH-k0h3Mf"
   },
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Translating Human Readable Dates Into Machine Readable Dates\n",
    "\n",
    "* The model you will build here could be used to translate from one language to another, such as translating from English to Hindi. \n",
    "* However, language translation requires massive datasets and usually takes days of training on GPUs. \n",
    "* To give you a place to experiment with these models without using massive datasets, we will perform a simpler \"date translation\" task. \n",
    "* The network will input a date written in a variety of possible formats (*e.g. \"the 29th of August 1958\", \"03/30/1968\", \"24 JUNE 1987\"*) \n",
    "* The network will translate them into standardized, machine readable dates (*e.g. \"1958-08-29\", \"1968-03-30\", \"1987-06-24\"*). \n",
    "* We will have the network learn to output dates in the common machine-readable format YYYY-MM-DD. \n",
    "\n",
    "<!-- \n",
    "Take a look at [nmt_utils.py](./nmt_utils.py) to see all the formatting. Count and figure out how the formats work, you will need this knowledge later. !--> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22b1893-2819-4e4d-bd00-6cd59d51b6d8",
   "metadata": {
    "id": "8BhEaJvph3Mf"
   },
   "source": [
    "<a name='1-1'></a>\n",
    "### 1.1 - Dataset\n",
    "\n",
    "We will train the model on a dataset of 10,000 human readable dates and their equivalent, standardized, machine readable dates. Let's run the following cells to load the dataset and print some examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "219d91f3-a64d-4a1c-a50e-a0e21a22a7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 10000/10000 [00:00<00:00, 16247.86it/s]\n"
     ]
    }
   ],
   "source": [
    "m = 10000\n",
    "dataset, human_vocab, machine_vocab, inv_machine_vocab = load_dataset(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c72295e0-a3c6-40ac-a897-c0eb531a252e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('8 jul 1992', '1992-07-08'),\n",
       " ('20.07.70', '1970-07-20'),\n",
       " ('8/7/14', '2014-08-07'),\n",
       " ('friday february 21 1986', '1986-02-21'),\n",
       " ('wednesday november 29 1989', '1989-11-29'),\n",
       " ('thursday june 19 1980', '1980-06-19'),\n",
       " ('friday august 4 2000', '2000-08-04'),\n",
       " ('27 sep 1978', '1978-09-27'),\n",
       " ('19 sep 1976', '1976-09-19'),\n",
       " ('wednesday may 26 1993', '1993-05-26')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d33e9f5-3072-4055-923b-cf501c709a52",
   "metadata": {
    "id": "ao4Ffrkxh3Mg"
   },
   "source": [
    "You've loaded:\n",
    "- `dataset`: a list of tuples of (human readable date, machine readable date).\n",
    "- `human_vocab`: a python dictionary mapping all characters used in the human readable dates to an integer-valued index.\n",
    "- `machine_vocab`: a python dictionary mapping all characters used in machine readable dates to an integer-valued index. \n",
    "    - **Note**: These indices are not necessarily consistent with `human_vocab`. \n",
    "- `inv_machine_vocab`: the inverse dictionary of `machine_vocab`, mapping from indices back to characters. \n",
    "\n",
    "Let's preprocess the data and map the raw text data into the index values. \n",
    "- We will set Tx=30 \n",
    "    - We assume Tx is the maximum length of the human readable date.\n",
    "    - If we get a longer input, we would have to truncate it.\n",
    "- We will set Ty=10\n",
    "    - \"YYYY-MM-DD\" is 10 characters long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f18c0a0a-d32e-4567-87c9-c32ee0a7e4c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 0,\n",
       " '.': 1,\n",
       " '/': 2,\n",
       " '0': 3,\n",
       " '1': 4,\n",
       " '2': 5,\n",
       " '3': 6,\n",
       " '4': 7,\n",
       " '5': 8,\n",
       " '6': 9,\n",
       " '7': 10,\n",
       " '8': 11,\n",
       " '9': 12,\n",
       " 'a': 13,\n",
       " 'b': 14,\n",
       " 'c': 15,\n",
       " 'd': 16,\n",
       " 'e': 17,\n",
       " 'f': 18,\n",
       " 'g': 19,\n",
       " 'h': 20,\n",
       " 'i': 21,\n",
       " 'j': 22,\n",
       " 'l': 23,\n",
       " 'm': 24,\n",
       " 'n': 25,\n",
       " 'o': 26,\n",
       " 'p': 27,\n",
       " 'r': 28,\n",
       " 's': 29,\n",
       " 't': 30,\n",
       " 'u': 31,\n",
       " 'v': 32,\n",
       " 'w': 33,\n",
       " 'y': 34,\n",
       " '<unk>': 35,\n",
       " '<pad>': 36}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e51acdd6-5d00-432f-ba3a-20b6c953cd5e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16962,
     "status": "ok",
     "timestamp": 1612468514157,
     "user": {
      "displayName": "Mubsi K",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gip7OjOkdNkKxKDyWEQAq1o8ccGN_HrBTGdqjgQ=s64",
      "userId": "08094225471505108399"
     },
     "user_tz": -300
    },
    "id": "Qdso90EBh3Mg",
    "outputId": "0a364ad8-8b25-4de3-f036-d5d8e40bdf8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (10000, 30)\n",
      "Y.shape: (10000, 10)\n",
      "Xoh.shape: torch.Size([10000, 30, 37])\n",
      "Yoh.shape: torch.Size([10000, 10, 11])\n"
     ]
    }
   ],
   "source": [
    "Tx = 30\n",
    "Ty = 10\n",
    "X, Y, Xoh, Yoh = preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "Xoh = Xoh.to(device)\n",
    "Yoh = Yoh.to(device)\n",
    "\n",
    "print(\"X.shape:\", X.shape)\n",
    "print(\"Y.shape:\", Y.shape)\n",
    "print(\"Xoh.shape:\", Xoh.shape)\n",
    "print(\"Yoh.shape:\", Yoh.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7f8d18-96d4-4ba1-8a11-930d74a99f27",
   "metadata": {
    "id": "q9C0UY25h3Mh"
   },
   "source": [
    "You now have:\n",
    "- `X`: a processed version of the human readable dates in the training set.\n",
    "    - Each character in X is replaced by an index (integer) mapped to the character using `human_vocab`. \n",
    "    - Each date is padded to ensure a length of $T_x$ using a special character (< pad >). \n",
    "    - `X.shape = (m, Tx)` where m is the number of training examples in a batch.\n",
    "- `Y`: a processed version of the machine readable dates in the training set.\n",
    "    - Each character is replaced by the index (integer) it is mapped to in `machine_vocab`. \n",
    "    - `Y.shape = (m, Ty)`. \n",
    "- `Xoh`: one-hot version of `X`\n",
    "    - Each index in `X` is converted to the one-hot representation (if the index is 2, the one-hot version has the index position 2 set to 1, and the remaining positions are 0.\n",
    "    - `Xoh.shape = (m, Tx, len(human_vocab))`\n",
    "- `Yoh`: one-hot version of `Y`\n",
    "    - Each index in `Y` is converted to the one-hot representation. \n",
    "    - `Yoh.shape = (m, Ty, len(machine_vocab))`. \n",
    "    - `len(machine_vocab) = 11` since there are 10 numeric digits (0 to 9) and the `-` symbol."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e00a67-ae18-45c0-ad6f-d3249179f7be",
   "metadata": {
    "id": "N7qKvWrTh3Mh"
   },
   "source": [
    "* Let's also look at some examples of preprocessed training examples. \n",
    "* Feel free to play with `index` in the cell below to navigate the dataset and see how source/target dates are preprocessed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a27f9f51-264b-43db-9548-35358a9d4f14",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16952,
     "status": "ok",
     "timestamp": 1612468514158,
     "user": {
      "displayName": "Mubsi K",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gip7OjOkdNkKxKDyWEQAq1o8ccGN_HrBTGdqjgQ=s64",
      "userId": "08094225471505108399"
     },
     "user_tz": -300
    },
    "id": "kUOayR4gh3Mh",
    "outputId": "d20994de-bbea-4cc7-ffaf-38a05974c9db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source date: 8 jul 1992\n",
      "Target date: 1992-07-08\n",
      "\n",
      "Source after preprocessing (indices): [11  0 22 31 23  0  4 12 12  5 36 36 36 36 36 36 36 36 36 36 36 36 36 36\n",
      " 36 36 36 36 36 36]\n",
      "Target after preprocessing (indices): [ 2 10 10  3  0  1  8  0  1  9]\n",
      "\n",
      "Source after preprocessing (one-hot): tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]], device='cuda:0')\n",
      "Target after preprocessing (one-hot): tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "print(\"Source date:\", dataset[index][0])\n",
    "print(\"Target date:\", dataset[index][1])\n",
    "print()\n",
    "print(\"Source after preprocessing (indices):\", X[index])\n",
    "print(\"Target after preprocessing (indices):\", Y[index])\n",
    "print()\n",
    "print(\"Source after preprocessing (one-hot):\", Xoh[index])\n",
    "print(\"Target after preprocessing (one-hot):\", Yoh[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85074a2c-c19a-4367-a576-dfbc0463ba76",
   "metadata": {
    "id": "94o4RYbOh3Mi"
   },
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Neural Machine Translation with Attention\n",
    "\n",
    "* If you had to translate a book's paragraph from French to English, you would not read the whole paragraph, then close the book and translate. \n",
    "* Even during the translation process, you would read/re-read and focus on the parts of the French paragraph corresponding to the parts of the English you are writing down. \n",
    "* The attention mechanism tells a Neural Machine Translation model where it should pay attention to at any step. \n",
    "\n",
    "<a name='2-1'></a>\n",
    "### 2.1 - Attention Mechanism\n",
    "\n",
    "In this part, you will implement the attention mechanism presented in the lecture videos. \n",
    "* Here is a figure to remind you how the model works. \n",
    "    * The diagram on the left shows the attention model. \n",
    "    * The diagram on the right shows what one \"attention\" step does to calculate the attention variables $\\alpha^{\\langle t, t' \\rangle}$.\n",
    "    * The attention variables $\\alpha^{\\langle t, t' \\rangle}$ are used to compute the context variable $context^{\\langle t \\rangle}$ for each timestep in the output ($t=1, \\ldots, T_y$). \n",
    "\n",
    "<table>\n",
    "<td> \n",
    "<img src=\"images/attn_model.png\" style=\"width:500;height:500px;\"> <br>\n",
    "</td> \n",
    "<td> \n",
    "<img src=\"images/attn_mechanism.png\" style=\"width:500;height:500px;\"> <br>\n",
    "</td> \n",
    "</table>\n",
    "<caption><center> **Figure 1**: Neural machine translation with attention</center></caption>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913939b6-116c-423f-84e4-eb65548ae2d7",
   "metadata": {
    "id": "b2TkQnykh3Mi"
   },
   "source": [
    "Here are some properties of the model that you may notice: \n",
    "\n",
    "#### Pre-attention and Post-attention LSTMs on both sides of the attention mechanism\n",
    "- There are two separate LSTMs in this model (see diagram on the left): pre-attention and post-attention LSTMs.\n",
    "- *Pre-attention* Bi-LSTM is the one at the bottom of the picture is a Bi-directional LSTM and comes *before* the attention mechanism.\n",
    "    - The attention mechanism is shown in the middle of the left-hand diagram.\n",
    "    - The pre-attention Bi-LSTM goes through $T_x$ time steps\n",
    "- *Post-attention* LSTM: at the top of the diagram comes *after* the attention mechanism. \n",
    "    - The post-attention LSTM goes through $T_y$ time steps. \n",
    "\n",
    "- The post-attention LSTM passes the hidden state $s^{\\langle t \\rangle}$ and cell state $c^{\\langle t \\rangle}$ from one time step to the next. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a4d58b-6588-40ca-bf21-29a910233f47",
   "metadata": {
    "id": "JpznWuWqh3Mi"
   },
   "source": [
    "#### An LSTM has both a hidden state and cell state\n",
    "* In the lecture videos, we were using only a basic RNN for the post-attention sequence model\n",
    "    * This means that the state captured by the RNN was outputting only the hidden state $s^{\\langle t\\rangle}$. \n",
    "* In this assignment, we are using an LSTM instead of a basic RNN.\n",
    "    * So the LSTM has both the hidden state $s^{\\langle t\\rangle}$ and the cell state $c^{\\langle t\\rangle}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f403b0b9-c4ad-4a90-a373-f81d8ff04c08",
   "metadata": {
    "id": "85btUzl4h3Mj"
   },
   "source": [
    "#### Each time step does not use predictions from the previous time step\n",
    "* Unlike previous text generation examples earlier in the course, in this model, the post-attention LSTM at time $t$ does not take the previous time step's prediction $y^{\\langle t-1 \\rangle}$ as input.\n",
    "* The post-attention LSTM at time 't' only takes the hidden state $s^{\\langle t\\rangle}$ and cell state $c^{\\langle t\\rangle}$ as input. \n",
    "* We have designed the model this way because **unlike language generation (where adjacent characters are highly correlated) there isn't as strong a dependency between the previous character and the next character in a YYYY-MM-DD date**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25743a6f-f5de-4bdb-aad6-0c70466aa882",
   "metadata": {
    "id": "NYT3v7rUh3Mk",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Concatenation of hidden states from the forward and backward pre-attention LSTMs\n",
    "- $\\overrightarrow{a}^{\\langle t \\rangle}$: hidden state of the forward-direction, pre-attention LSTM.\n",
    "- $\\overleftarrow{a}^{\\langle t \\rangle}$: hidden state of the backward-direction, pre-attention LSTM.\n",
    "- $a^{\\langle t \\rangle} = [\\overrightarrow{a}^{\\langle t \\rangle}, \\overleftarrow{a}^{\\langle t \\rangle}]$: the concatenation of the activations of both the forward-direction $\\overrightarrow{a}^{\\langle t \\rangle}$ and backward-directions $\\overleftarrow{a}^{\\langle t \\rangle}$ of the pre-attention Bi-LSTM. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac48558-cc7e-4156-a887-390239a37f9d",
   "metadata": {
    "id": "97GUKCqwh3Mk"
   },
   "source": [
    "#### Computing \"energies\" $e^{\\langle t, t' \\rangle}$ as a function of $s^{\\langle t-1 \\rangle}$ and $a^{\\langle t' \\rangle}$\n",
    "- Recall in the lesson videos \"Attention Model\", at time 6:45 to 8:16, the definition of \"e\" as a function of $s^{\\langle t-1 \\rangle}$ and $a^{\\langle t \\rangle}$.\n",
    "    - \"e\" is called the \"energies\" variable.\n",
    "    - $s^{\\langle t-1 \\rangle}$ is the hidden state of the post-attention LSTM\n",
    "    - $a^{\\langle t' \\rangle}$ is the hidden state of the pre-attention LSTM.\n",
    "    - $s^{\\langle t-1 \\rangle}$ and $a^{\\langle t \\rangle}$ are fed into a simple neural network, which learns the function to output $e^{\\langle t, t' \\rangle}$.\n",
    "    - $e^{\\langle t, t' \\rangle}$ is then used when computing the attention $a^{\\langle t, t' \\rangle}$ that $y^{\\langle t \\rangle}$ should pay to $a^{\\langle t' \\rangle}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f096565-3755-49df-83fe-9bf84189f3a5",
   "metadata": {
    "id": "scu_HnPNh3Mk"
   },
   "source": [
    "- The diagram on the right of figure 1 uses a `RepeatVector` node to copy $s^{\\langle t-1 \\rangle}$'s value $T_x$ times.\n",
    "- Then it uses `Concatenation` to concatenate $s^{\\langle t-1 \\rangle}$ and $a^{\\langle t \\rangle}$.\n",
    "- The concatenation of $s^{\\langle t-1 \\rangle}$ and $a^{\\langle t \\rangle}$ is fed into a \"Dense\" layer, which computes $e^{\\langle t, t' \\rangle}$. \n",
    "- $e^{\\langle t, t' \\rangle}$ is then passed through a softmax to compute $\\alpha^{\\langle t, t' \\rangle}$.\n",
    "- Note that the diagram doesn't explicitly show variable $e^{\\langle t, t' \\rangle}$, but $e^{\\langle t, t' \\rangle}$ is above the Dense layer and below the Softmax layer in the diagram in the right half of figure 1.\n",
    "- We'll explain how to use `RepeatVector` and `Concatenation` in Keras below. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc9ecae-d71f-4bfc-b246-a58420ce0621",
   "metadata": {
    "id": "_ukmqe_Yh3Ml"
   },
   "source": [
    "#### Implementation Details\n",
    "   \n",
    "Let's implement this neural translator. You will start by implementing two functions: `one_step_attention()` and `model()`.\n",
    "\n",
    "#### one_step_attention\n",
    "* The inputs to the one_step_attention at time step $t$ are:\n",
    "    - $[a^{<1>},a^{<2>}, ..., a^{<T_x>}]$: all hidden states of the pre-attention Bi-LSTM.\n",
    "    - $s^{<t-1>}$: the previous hidden state of the post-attention LSTM \n",
    "* one_step_attention computes:\n",
    "    - $[\\alpha^{<t,1>},\\alpha^{<t,2>}, ..., \\alpha^{<t,T_x>}]$: the attention weights\n",
    "    - $context^{ \\langle t \\rangle }$: the context vector:\n",
    "    \n",
    "$$context^{<t>} = \\sum_{t' = 1}^{T_x} \\alpha^{<t,t'>}a^{<t'>}\\tag{1}$$ \n",
    "\n",
    "##### Clarifying 'context' and 'c'\n",
    "- In the lecture videos, the context was denoted $c^{\\langle t \\rangle}$\n",
    "- In the assignment, we are calling the context $context^{\\langle t \\rangle}$.\n",
    "    - This is to avoid confusion with the post-attention LSTM's internal memory cell variable, which is also denoted $c^{\\langle t \\rangle}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bae90d-5418-46d2-b1c4-e06aa0302bea",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "591f4314-2c2e-4802-bfcd-c1e48ab0d171",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionMechanism(nn.Module):\n",
    "    def __init__(self, encoder_hidden_size, decoder_hidden_size):\n",
    "        super(AttentionMechanism, self).__init__()\n",
    "\n",
    "        # Linear layers\n",
    "        self.attn_layer1 = nn.Linear(encoder_hidden_size * 2 + decoder_hidden_size, 10)\n",
    "        self.attn_layer2 = nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, encoder_outputs, decoder_hidden):\n",
    "        \"\"\"\n",
    "        encoder_outputs -- output from the encoder Bi-LSTM, shape: (batch_size, sequence_length, 2 * encoder_hidden_size)\n",
    "        decoder_hidden -- last hidden state of the decoder LSTM, shape: (batch_size, decoder_hidden_size)\n",
    "        \"\"\"\n",
    "        # Get the sequence length and batch size from encoder outputs\n",
    "        seq_len = encoder_outputs.size(1)\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "\n",
    "        # Repeat decoder hidden state to match sequence length\n",
    "        decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1, seq_len, 1)\n",
    "\n",
    "        # Concatenate encoder outputs with repeated decoder hidden state\n",
    "        concatenated_inputs = torch.cat((encoder_outputs, decoder_hidden), dim=2)\n",
    "\n",
    "        # Apply two linear layers with a tanh activation in between\n",
    "        attn_energies = self.attn_layer1(concatenated_inputs)\n",
    "        attn_energies = torch.tanh(attn_energies)\n",
    "        attn_energies = self.attn_layer2(attn_energies).squeeze(2)\n",
    "        # attn_energies = nn.ReLU()(attn_energies)\n",
    "\n",
    "        # Apply softmax to calculate attention weights\n",
    "        attention_weights = F.softmax(attn_energies, dim=1).unsqueeze(1)\n",
    "\n",
    "        # Compute context vector as weighted sum of encoder outputs\n",
    "        context_vector = torch.bmm(attention_weights, encoder_outputs).squeeze(1)\n",
    "\n",
    "        # attention_weights are also returned for visualization\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b377c034-1332-42ae-9375-16e9e3b7216d",
   "metadata": {},
   "source": [
    "### Encoder and Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d2753248-3768-4a5a-a951-9d2c138d4fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, bidirectional=True):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, bidirectional=bidirectional, batch_first=True)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        outputs, (hidden, cell) = self.lstm(input_seq)\n",
    "        return outputs, (hidden, cell)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)  # final linear layer (logits)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        output, (hidden, cell) = self.lstm(input, (hidden, cell))\n",
    "        output = self.linear(output)\n",
    "        return output, (hidden, cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f14b1c-5067-400b-9495-d069ecfef791",
   "metadata": {},
   "source": [
    "### Forward test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb27d59-c800-4058-b1dc-d294b18836ac",
   "metadata": {},
   "source": [
    "Let's run a step-by-step test to see whether encoder, decoder, and attention will work as what we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "058b8cde-b8d7-4986-9b4a-b51d69d96eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Encoder and Decoder\n",
    "encoder = Encoder(input_size=37, hidden_size=32).to(device)\n",
    "decoder = Decoder(input_size=64, hidden_size=64, output_size=11).to(device)\n",
    "attention = AttentionMechanism(encoder_hidden_size=32, decoder_hidden_size=64).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34dd7f4-aa0e-434e-af05-5ae4cdba648b",
   "metadata": {},
   "source": [
    "Take first 3 examples so that batch size is 3, sequence length is 30 and vocabulary size is 37."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "88d6202f-9659-4816-8471-fed470e2fc61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 30, 37])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xoh[:3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c2524a32-5e89-4426-9e50-cfc42515ea6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_outputs, (hidden, cell) = encoder(Xoh[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1d35d75b-5f25-4aec-8f2a-63cb0406faf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder output shape: torch.Size([3, 30, 64])\n",
      "encoder hidden shape: torch.Size([2, 3, 32])\n",
      "encoder cell shape: torch.Size([2, 3, 32])\n"
     ]
    }
   ],
   "source": [
    "print(f'encoder output shape: {encoder_outputs.shape}')\n",
    "print(f'encoder hidden shape: {hidden.shape}')\n",
    "print(f'encoder cell shape: {cell.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "16ff23f3-18d6-4d13-b34a-dfe6835d7ea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 64])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_hidden.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f3b387-2d9f-431d-bb86-f80f0272e7b5",
   "metadata": {},
   "source": [
    "Here `hidden` and `cell` has its first dimension 2 because the encoder is a bidirectional LSTM. Below we initialize the decoder's hidden and cell states using the same shape except that the first dimension is flattened, as the decoder is one-direction LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4c771242-53e4-4ece-9cbf-767c19ff33c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use zero vectors as decoder's initial states\n",
    "decoder_hidden = torch.zeros_like(hidden).view(1, 3, -1).to(device)  # Reshape to [1, batch_size, hidden_size]\n",
    "decoder_cell = torch.zeros_like(cell).view(1, 3, -1).to(device) # Reshape to [1, batch_size, hidden_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "063c27ff-573a-48dd-ad65-0e7bf0067748",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 64])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Forward pass through the attention mechanism\n",
    "context_vector, _ = attention(encoder_outputs, decoder_hidden.squeeze(0))\n",
    "context_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "22ba943a-e736-4304-b395-2d6331e7fd61",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder output (one-step) shape: torch.Size([3, 1, 11])\n",
      "decoder hidden shape: torch.Size([1, 3, 64])\n",
      "decoder cell shape: torch.Size([1, 3, 64])\n"
     ]
    }
   ],
   "source": [
    "decoder_output, (decoder_hidden, decoder_cell) = decoder(context_vector.unsqueeze(1), decoder_hidden, decoder_cell)\n",
    "decoder_output.shape\n",
    "print(f'decoder output (one-step) shape: {decoder_output.shape}')\n",
    "print(f'decoder hidden shape: {decoder_hidden.shape}')\n",
    "print(f'decoder cell shape: {decoder_cell.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa143ec1-1a71-4bce-8720-4ecdc1013a4f",
   "metadata": {},
   "source": [
    "For every forward step of the decoder, the attention is calculated using decoder's hidden state of the previous step, so the forward process must be implemented sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "01ba50c2-2d41-48c6-9db2-f905a60386ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequences(encoder_outputs, decoder, attention, target_length, start_token):\n",
    "    batch_size = encoder_outputs.size(0)\n",
    "    decoder_input = start_token  # Initialize with start token\n",
    "    decoder_hidden, decoder_cell = torch.zeros(1, batch_size, decoder.lstm.hidden_size).to(device), torch.zeros(1, batch_size, decoder.lstm.hidden_size).to(device)\n",
    "\n",
    "    decoder_outputs = []\n",
    "    for t in range(target_length):\n",
    "        context_vector, _ = attention(encoder_outputs, decoder_hidden.squeeze(0))\n",
    "        decoder_output, (decoder_hidden, decoder_cell) = decoder(context_vector.unsqueeze(1), decoder_hidden, decoder_cell)\n",
    "        decoder_outputs.append(decoder_output)\n",
    "\n",
    "    return torch.cat(decoder_outputs, dim=1)  # Concatenate along the time dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0526d81f-4a2f-498a-83ca-1c73da962526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10, 11])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_string = decode_sequences(encoder_outputs, decoder, attention, target_length=10, start_token=context_vector.unsqueeze(1))\n",
    "target_string.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bb1a194a-7a4e-407f-80d4-a43f83fa9679",
   "metadata": {},
   "outputs": [],
   "source": [
    "del encoder, decoder, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a8a173-f689-4ede-9ad6-440086ab60b7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Wrapping-Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f92757-95e0-469b-8a57-9052c76ed401",
   "metadata": {},
   "source": [
    "Now we are ready to make a new class to contain the whole model, so that we can run the forward process in a simple manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "95b8bfe2-6df4-4592-8de8-ec8e2ab91ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqAttentionModel(nn.Module):\n",
    "    def __init__(self, encoder, decoder, attention, device):\n",
    "        super(Seq2SeqAttentionModel, self).__init__()\n",
    "        self.encoder = encoder.to(device)\n",
    "        self.decoder = decoder.to(device)\n",
    "        self.attention = attention.to(device)\n",
    "\n",
    "    def forward(self, source, target_length, start_token=None):\n",
    "        batch_size = source.size(0)\n",
    "        encoder_outputs, (hidden, cell) = self.encoder(source)\n",
    "        attention_weights_list = []\n",
    "        \n",
    "        # Initialize decoder hidden and cell states\n",
    "        decoder_hidden = torch.zeros(1, batch_size, self.decoder.lstm.hidden_size).to(device)\n",
    "        decoder_cell = torch.zeros(1, batch_size, self.decoder.lstm.hidden_size).to(device)\n",
    "\n",
    "        # Initialize the decoder input as the start token\n",
    "        if start_token == None:\n",
    "            # Compute initial context vector and set as initial decoder input\n",
    "            context_vector, _ = self.attention(encoder_outputs, decoder_hidden.squeeze(0))\n",
    "            start_token = context_vector.unsqueeze(1)\n",
    "        decoder_input = start_token\n",
    "\n",
    "        # Store the decoder outputs\n",
    "        decoder_outputs = []\n",
    "\n",
    "        for t in range(target_length):\n",
    "            context_vector, attention_weights = self.attention(encoder_outputs, decoder_hidden.squeeze(0))\n",
    "            decoder_output, (decoder_hidden, decoder_cell) = self.decoder(context_vector.unsqueeze(1), decoder_hidden, decoder_cell)\n",
    "            decoder_outputs.append(decoder_output)\n",
    "            attention_weights_list.append(attention_weights)\n",
    "\n",
    "        return torch.cat(decoder_outputs, dim=1), torch.cat(attention_weights_list, dim=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284973f9-5e6d-481b-b27c-84e9950afacc",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2499116d-33a0-439f-91aa-5470d68a4fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "class DateTranslationDataset(Dataset):\n",
    "    def __init__(self, Xoh, Yoh):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            Xoh (Tensor): Tensor of shape (batch_size, sequence_length, input_vocab_size)\n",
    "            Yoh (Tensor): Tensor of shape (batch_size, sequence_length, output_vocab_size)\n",
    "        \"\"\"\n",
    "        self.Xoh = Xoh\n",
    "        self.Yoh = Yoh\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.Xoh)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.Xoh[idx], self.Yoh[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bd465128-7497-480b-807a-7675b89ebd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DateTranslationDataset(Xoh, Yoh)\n",
    "batch_size = 512\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "encoder = Encoder(input_size=37, hidden_size=32)\n",
    "decoder = Decoder(input_size=64, hidden_size=64, output_size=11)\n",
    "attention = AttentionMechanism(encoder_hidden_size=32, decoder_hidden_size=64)\n",
    "seq2seq_model = Seq2SeqAttentionModel(encoder, decoder, attention, device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(seq2seq_model.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c6b4f30e-734b-4017-b19c-8a2b6854d4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Optimizers for encoder, decoder, and attention\n",
    "# encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.005)\n",
    "# decoder_optimizer = optim.Adam(list(decoder.parameters()) + list(attention.parameters()), lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "97154086-131a-4b72-9c3d-5b147d71e670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Other initializations...\n",
    "# num_epochs = 20\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     total_loss = 0\n",
    "\n",
    "#     for input_batch, target_batch in data_loader:\n",
    "#         # Zero the gradients\n",
    "#         encoder_optimizer.zero_grad(set_to_none=True)\n",
    "#         decoder_optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "#         # Forward pass through encoder\n",
    "#         encoder_outputs, (hidden, cell) = encoder(input_batch)\n",
    "\n",
    "#         # Prepare initial decoder hidden and cell states\n",
    "#         decoder_hidden, decoder_cell = torch.zeros(1, input_batch.size(0), decoder.lstm.hidden_size), torch.zeros(1, input_batch.size(0), decoder.lstm.hidden_size)\n",
    "\n",
    "#         # Compute initial context vector and set as initial decoder input\n",
    "#         context_vector = attention(encoder_outputs, decoder_hidden.squeeze(0))\n",
    "#         decoder_input = context_vector.unsqueeze(1)\n",
    "\n",
    "#         # Forward pass through the decoder for each time step\n",
    "#         decoder_outputs = []\n",
    "#         for t in range(target_batch.size(1)):  # target_batch.size(1) is the sequence length\n",
    "#             decoder_output, (decoder_hidden, decoder_cell) = decoder(decoder_input, decoder_hidden, decoder_cell)\n",
    "#             decoder_outputs.append(decoder_output)\n",
    "#             context_vector = attention(encoder_outputs, decoder_hidden.squeeze(0))\n",
    "#             decoder_input = context_vector.unsqueeze(1)\n",
    "\n",
    "#         decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "\n",
    "#         # Compute the loss\n",
    "#         loss = criterion(decoder_outputs.transpose(1, 2), target_batch.argmax(dim=2))\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#         # Perform backpropagation and optimize\n",
    "#         loss.backward()\n",
    "#         encoder_optimizer.step()\n",
    "#         decoder_optimizer.step()\n",
    "\n",
    "#     print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(data_loader):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "33b1fca3-febd-4897-bf03-62061507a6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 2.1878\n",
      "Epoch [2/20], Loss: 1.9555\n",
      "Epoch [3/20], Loss: 1.5362\n",
      "Epoch [4/20], Loss: 1.2324\n",
      "Epoch [5/20], Loss: 1.2309\n",
      "Epoch [6/20], Loss: 1.1717\n",
      "Epoch [7/20], Loss: 1.0993\n",
      "Epoch [8/20], Loss: 0.9998\n",
      "Epoch [9/20], Loss: 0.9420\n",
      "Epoch [10/20], Loss: 0.8991\n",
      "Epoch [11/20], Loss: 0.8459\n",
      "Epoch [12/20], Loss: 0.7724\n",
      "Epoch [13/20], Loss: 0.6696\n",
      "Epoch [14/20], Loss: 0.5251\n",
      "Epoch [15/20], Loss: 0.3403\n",
      "Epoch [16/20], Loss: 0.1791\n",
      "Epoch [17/20], Loss: 0.0752\n",
      "Epoch [18/20], Loss: 0.0349\n",
      "Epoch [19/20], Loss: 0.0192\n",
      "Epoch [20/20], Loss: 0.0132\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "\n",
    "    for input_batch, target_batch in data_loader:\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        outputs, _ = seq2seq_model(input_batch, target_length=target_batch.size(1))\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs.transpose(1, 2), target_batch.argmax(dim=2))\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Perform backpropagation and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(data_loader):.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cad2323-b0dc-4087-a496-5cb6b3e97c9f",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "ae9538e9-c145-4b8f-bbd9-80323a027836",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 37])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = '1 Thursday, Aug. 1969'\n",
    "source = torch.tensor(string_to_int(sentence, 30, human_vocab))\n",
    "source = F.one_hot(source, num_classes=37).float().to(device)\n",
    "source.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "c63e941e-25ea-45ae-ae73-e91ba4f48f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1996-08-07\n"
     ]
    }
   ],
   "source": [
    "seq2seq_model.eval()\n",
    "output_string, attention_weights = seq2seq_model(source.unsqueeze(0), 10)\n",
    "output_string = torch.argmax(output_string, dim=2)\n",
    "output_string = ''.join([inv_machine_vocab[int(i)] for i in output_string.squeeze()])\n",
    "print(output_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "0cca53cd-b169-47ab-9ea7-fe9c1fb71018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 30])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "12502d99-8cea-4b9b-8e50-fd864270204c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_attention(attention_weights, input_sequence, output_sequence):\n",
    "    # Convert attention weights to numpy array\n",
    "    attention_weights = attention_weights.squeeze().cpu().detach().numpy()\n",
    "\n",
    "    # Define figure size and axis labels\n",
    "    plt.figure(figsize=(8,4))\n",
    "    sns.heatmap(attention_weights, xticklabels=input_sequence, yticklabels=output_sequence, cmap='viridis')\n",
    "\n",
    "    plt.xlabel('Input Sequence')\n",
    "    plt.ylabel('Output Sequence')\n",
    "    plt.title('Attention Weights')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Assuming input_sequence and output_sequence are lists of tokens/words in your sequences\n",
    "plot_attention(attention_weights, sentence, output_string)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffbf7ae-256a-48cc-b960-707928997d5c",
   "metadata": {},
   "source": [
    "The attention map looks kind of strange, maybe bugs exist."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
